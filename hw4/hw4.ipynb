{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ 4\n",
    "\n",
    "- Студент: Алексей Ярошенко\n",
    "- Email на портале: aleksey.yaroshenko@gmail.com\n",
    "- https://docs.google.com/document/d/1z35Z-xctrrYO8r6KD10w8daKCLEMkCBQW9iNk2nEskk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1729,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import everygrams\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Реализуйте базовый частотный метод по Шерлоку Холмсу\n",
    "\n",
    "- подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "- возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе совсем вряд ли сработает), зашифруйте их посредством случайной перестановки символов;\n",
    "- расшифруйте их таким частотным методом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "abc = ' абвгдежзийклмнопрстуфхцчшщъыьэюя'\n",
    "\n",
    "def tokenize(text, tokenizer=tokenizer):\n",
    "    text = ''.join([c for c in text if c.lower() in abc])\n",
    "    return ' '.join(tokenizer.tokenize(text.lower()))\n",
    "\n",
    "\n",
    "def get_freqs(text, min_freq=0, n_gram=1):\n",
    "    freqs = dict()\n",
    "    if n_gram > 1:\n",
    "        text = [''.join(ngram) for ngram in everygrams(text, min_len=n_gram, max_len=n_gram)]\n",
    "    for key, value in Counter(text).items():\n",
    "        if value/len(text) > min_freq:\n",
    "            freqs[key] = value/len(text)\n",
    "    return freqs\n",
    "\n",
    "\n",
    "def generate_mapping(freqs):\n",
    "    original = list(freqs.keys())\n",
    "    replacements = np.random.choice(original, replace=False, size=len(freqs))\n",
    "    mapping = dict()\n",
    "    for original_char, replacement_char in zip(original, replacements):\n",
    "        mapping[original_char] = replacement_char\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def apply_mapping(text, mapping):\n",
    "    return ''.join([mapping.get(c, 'ь') for c in text])\n",
    "\n",
    "# Здесь мы находим ближайший по частотности символ.\n",
    "# Пробовал использовать просто ранги, но качество хуже.\n",
    "def get_reverse_mapping(corpus_freqs, text_freqs):\n",
    "    corpus_freqs_sorted = sorted(corpus_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    text_freqs_sorted = sorted(text_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    reverse_mapping = dict()\n",
    "    for text_char, text_freq in text_freqs_sorted:\n",
    "        min_diff = 1\n",
    "        best_char = None\n",
    "        for corpus_char, corpus_freq in corpus_freqs_sorted:\n",
    "            diff = abs(corpus_freq - text_freq)\n",
    "            if diff < min_diff:\n",
    "                best_char = corpus_char\n",
    "                min_diff = diff\n",
    "\n",
    "        reverse_mapping[text_char] = best_char\n",
    "        corpus_freqs_sorted = [(char, freq) for char, freq in corpus_freqs_sorted if char !=best_char]\n",
    "        \n",
    "    return reverse_mapping\n",
    "\n",
    "\n",
    "def char_accuracy(text1, text2):\n",
    "    assert len(text1) == len(text2)\n",
    "    right_chars = 0\n",
    "    for c1, c2 in zip(text1, text2):\n",
    "        right_chars += int(c1 == c2)\n",
    "    return right_chars / len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1693,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "with open('corpora/AnnaKarenina.txt' ,'r') as file:\n",
    "    corpus1 = file.readlines()\n",
    "    \n",
    "with open('corpora/WarAndPeace.txt' ,'r') as file:\n",
    "    corpus2 = file.readlines()\n",
    "    \n",
    "corpus = corpus1 + corpus2\n",
    "\n",
    "tokenized_corpus = tokenize(' '.join(corpus))\n",
    "corpus_freqs = get_freqs(tokenized_corpus)\n",
    "mapping = generate_mapping(corpus_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в течение многих часов шерлок холмс сидел согнувшись над стеклянной пробиркой в которой варилось чтото на редкость вонючее голова его была опущена на грудь и он казался мне похожим на странную товцую птицу с тусклыми серыми перьями и черным хохолком итак уотсон сказал он внезапно вы не собираетесь вкладывать свои сбережения в южноафриканские ценные бумаги я вздрогнул от удивления как ни привык я к необычайным способностям холмса это внезапное вторжение в самые тайные мои мысли было совершенно необъяснимым как черт возьми вы об этом узнали спросил я он повернулся на стуле держа в руке дымящуюся пробирку и его глубоко сидящие глаза радостно заблистали признайтесь уотсон что вы совершенно сбиты с толку сказал он признаюсь мне следовало бы заставить вас написать об этом на листочке бумаги и подписаться почему потому что через пять минут вы скажете что все это необычайно просто'"
      ]
     },
     "execution_count": 1694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('corpora/text.txt' ,'r') as file:\n",
    "    text = file.readlines()\n",
    "\n",
    "tokenized_text = tokenize(' '.join(text))\n",
    "encoded_text = apply_mapping(tokenized_text, mapping)\n",
    "text_freqs = get_freqs(encoded_text)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пумзкзвчзулвшючхукщршпуфздншьухшнлрурчызнуршювипфчр увщыурмзьнэввшбугдшсчдьшбупуьшмшдшбупщдчншр укмшмшувщудзыьшрм упшвокззуюшншпщузюшусенщушгитзвщувщуюдиы учушвуьщащнрэулвзугшхшжчлувщурмдщввиоумшпъиоугмчъиурумирьнелчурзделчугзд элчучукздвелухшхшньшлучмщьуишмршвурьщащнушвупвзащгвшупеувзуршсчдщзмзр упьнщыепщм урпшчурсздзжзвчэупуожвшщйдчьщврьчзуъзввезусилщючуэупаыдшювинушмуиычпнзвчэуьщьувчугдчпеьуэуьувзшсекщбвелургшршсвшрмэлухшнлрщуямшупвзащгвшзупмшджзвчзупурщлезумщбвезулшчулернчусеншуршпздфзввшувзшсцэрвчлелуьщьукздмупша лчупеушсуямшлуиавщнчургдшрчнуэушвугшпздвинрэувщурминзуызджщупудиьзуыелэтиорэугдшсчдьиучузюшуюнисшьшурчыэтчзуюнщащудщышрмвшуащснчрмщнчугдчавщбмзр уишмршвукмшупеуршпздфзввшурсчмеурумшньиурьщащнушвугдчавщор улвзурнзышпщншусеуащрмщпчм упщрувщгчрщм ушсуямшлувщунчрмшкьзусилщючучугшыгчрщм рэугшкзлиугшмшлиукмшукздзаугэм улчвимупеурьщжзмзукмшупрзуямшувзшсекщбвшугдшрмш'"
      ]
     },
     "execution_count": 1695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'р лнынасн даожсх ытиор энвком хокди исчнк иожаурэсиб атч илнмкгааош явоьсвмош р моловош ртвскоиб ылоло ат внчмоилб роаюынн жокорт нжо ьпкт ояущнат ат жвучб с оа мтзткиг дан яохойсд ат илвтааую лорфую ялсфу и луимкпдс инвпдс янвбгдс с ынвапд хохокмод слтм уолиоа имтзтк оа ранзтяао рп ан иоьсвтнлниб рмктчпртлб ирос иьнвнйнасг р юйаотъвсмтаимсн фнаапн ьудтжс г рзчвожаук ол учсркнасг мтм ас явсрпм г м аноьпытшапд ияоиоьаоилгд хокдит цло ранзтяаон рловйнасн р итдпн лтшапн дос дпикс ьпко иорнвэнаао аноьегиасдпд мтм ынвл розбдс рп оь цлод узаткс иявоиск г оа яорнваукиг ат илукн чнвйт р вумн чпдгщуюиг явоьсвму с нжо жкуьомо исчгщсн жктзт втчоилао зтьксилткс явсзатшлниб уолиоа ыло рп иорнвэнаао иьслп и локму имтзтк оа явсзатюиб дан икнчортко ьп зтилтрслб рти атяситлб оь цлод ат ксилоымн ьудтжс с яочяситлбиг яоынду яолоду ыло ынвнз яглб дсаул рп имтйнлн ыло рин цло аноьпытшао явоило'"
      ]
     },
     "execution_count": 1696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_mapping = get_reverse_mapping(corpus_freqs, text_freqs)\n",
    "decoded_text = apply_mapping(encoded_text, reverse_mapping)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31186440677966104"
      ]
     },
     "execution_count": 1697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(tokenized_text, decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем, так себе разультат :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Базовый частотный метод с биграммами\n",
    "\n",
    "\n",
    "Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "\n",
    "- подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "- проведите тестирование аналогично п.1, но при помощи биграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1698,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_freqs_bigram = get_freqs(tokenized_corpus, n_gram=2)\n",
    "text_freqs_bigram = get_freqs(encoded_text, n_gram=2)\n",
    "\n",
    "corpus_freqs_sorted = sorted(corpus_freqs_bigram.items(), key=lambda x: x[1], reverse=True)\n",
    "text_freqs_sorted = sorted(text_freqs_bigram.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1699,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь мы проходимся по биграмам, начиная от самых частотных, \n",
    "# на каждом шаге учитывая раскодированные ранее символы\n",
    "\n",
    "def get_reverse_mapping_bigram(corpus_freqs, text_freqs, init_mapping=None):\n",
    "    corpus_freqs_sorted = sorted(corpus_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    text_freqs_sorted = sorted(text_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if init_mapping is None:\n",
    "        init_mapping = []\n",
    "    reverse_mapping = {k: v for k, v in init_mapping}\n",
    "    \n",
    "    for i, (text_bigram, text_freq) in enumerate(text_freqs_sorted):\n",
    "\n",
    "        filtred_freqs = copy(corpus_freqs_sorted)\n",
    "        \n",
    "        if text_bigram[0] in reverse_mapping.keys():\n",
    "            filtred_freqs = [(bigram, freq) for bigram, freq in filtred_freqs if bigram[0] == reverse_mapping[text_bigram[0]]]\n",
    "            \n",
    "        if text_bigram[1] in reverse_mapping.keys():\n",
    "            filtred_freqs = [(bigram, freq) for bigram, freq in filtred_freqs if bigram[1] == reverse_mapping[text_bigram[1]]]\n",
    "              \n",
    "        min_diff = 1\n",
    "        best_bigram = None\n",
    "        for bigram, freq in filtred_freqs:\n",
    "            diff = abs(freq - text_freq)\n",
    "            if diff < min_diff:\n",
    "                best_bigram = bigram\n",
    "                min_diff = diff\n",
    "                \n",
    "        if text_bigram[0] not in reverse_mapping.keys():\n",
    "            reverse_mapping[text_bigram[0]] = best_bigram[0]\n",
    "            \n",
    "        if text_bigram[1] not in reverse_mapping.keys():\n",
    "            reverse_mapping[text_bigram[1]] = best_bigram[1]\n",
    "\n",
    "        \n",
    "    return reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в течение многих часов шерлок холмс сидел согнувшись над стеклянной пробиркой в которой варилось чтото на редкость вонючее голова его была опущена на грудь и он казался мне похожим на странную товцую птицу с тусклыми серыми перьями и черным хохолком итак уотсон сказал он внезапно вы не собираетесь вкладывать свои сбережения в южноафриканские ценные бумаги я вздрогнул от удивления как ни привык я к необычайным способностям холмса это внезапное вторжение в самые тайные мои мысли было совершенно необъяснимым как черт возьми вы об этом узнали спросил я он повернулся на стуле держа в руке дымящуюся пробирку и его глубоко сидящие глаза радостно заблистали признайтесь уотсон что вы совершенно сбиты с толку сказал он признаюсь мне следовало бы заставить вас написать об этом на листочке бумаги и подписаться почему потому что через пять минут вы скажете что все это необычайно просто'"
      ]
     },
     "execution_count": 1700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пумзкзвчзулвшючхукщршпуфздншьухшнлрурчызнуршювипфчр увщыурмзьнэввшбугдшсчдьшбупуьшмшдшбупщдчншр укмшмшувщудзыьшрм упшвокззуюшншпщузюшусенщушгитзвщувщуюдиы учушвуьщащнрэулвзугшхшжчлувщурмдщввиоумшпъиоугмчъиурумирьнелчурзделчугзд элчучукздвелухшхшньшлучмщьуишмршвурьщащнушвупвзащгвшупеувзуршсчдщзмзр упьнщыепщм урпшчурсздзжзвчэупуожвшщйдчьщврьчзуъзввезусилщючуэупаыдшювинушмуиычпнзвчэуьщьувчугдчпеьуэуьувзшсекщбвелургшршсвшрмэлухшнлрщуямшупвзащгвшзупмшджзвчзупурщлезумщбвезулшчулернчусеншуршпздфзввшувзшсцэрвчлелуьщьукздмупша лчупеушсуямшлуиавщнчургдшрчнуэушвугшпздвинрэувщурминзуызджщупудиьзуыелэтиорэугдшсчдьиучузюшуюнисшьшурчыэтчзуюнщащудщышрмвшуащснчрмщнчугдчавщбмзр уишмршвукмшупеуршпздфзввшурсчмеурумшньиурьщащнушвугдчавщор улвзурнзышпщншусеуащрмщпчм упщрувщгчрщм ушсуямшлувщунчрмшкьзусилщючучугшыгчрщм рэугшкзлиугшмшлиукмшукздзаугэм улчвимупеурьщжзмзукмшупрзуямшувзшсекщбвшугдшрмш'"
      ]
     },
     "execution_count": 1701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' оатнтнттонн гтвона   оитон ков нн о тстно  гн  ит  онасо аткнннн но о кток но ок а о но аотн   она а онаоотск  а о  нлнттог н  аотг окенао   ятнаонаого с ото нокакан ноннто  в итнонао аоанн лоа  х ло атх о оа  кненто тоенто то ннтотонтоненов в нк нотаако  а  но какано но нтка н о еонто  ктоатат  о кнасе аа о   то ктотитнтно олин ажоткан кттохтннеток нагтоно ксо гн но ао ст нтнтнокаконто от еконоконт кенаннено     кн  аннов нн аога о нтка н то а оитнтто о анетоааннетон тоне нтокен о   тоитнн онт кнн нтненокаконтоао  к нто ео кога но кнанто  о  тноно но   тон н нонао а нтостоиао оо ктосення л но о кток ототг огн к к о тсняттогнакаооас  ан окакнт аанто откнанат  о  а  нона о ео   тоитнн о ктаео оа нк о какано но откнал  оннто нтс  ан океока аа та о а она т аа о кога нонаонт а нкток нагтото  с т аа  но  нтн о  а н она онтотко на онтн ао ео каитатона о  тога онт кенанн о о  а '"
      ]
     },
     "execution_count": 1702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_mapping_bigram = get_reverse_mapping_bigram(corpus_freqs_bigram, text_freqs_bigram)\n",
    "decoded_text = apply_mapping(encoded_text, reverse_mapping_bigram)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16497175141242937"
      ]
     },
     "execution_count": 1703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(tokenized_text, decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало хуже, что неудивительно. Биграмм больше, попасть по частоте в правильные - сложнее. Нужен текст сильно больше.\n",
    "\n",
    "Попробуем добавить 1-2 раскодированных с помощью посимвольного метода символа как инициализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('у', ' '), ('ш', 'о')]"
      ]
     },
     "execution_count": 1704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_mapping = list(reverse_mapping.items())[:2]\n",
    "init_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'с такаоаа ьооеал кнсос  алиоо лоиьс саыаи соеорс ася оны стаоиьооо  слоналоо  с оотоло  снлаиося ктото он лаыоостя сооукаа еоиосн аео нтин осркаон он елрыя а оо ононись ьоа соло аь он стлноору тосару стаар с трсоитьа салтьа саляььа а калоть лолоиооь атно ротсоо сонони оо соаонсоо ст оа соналнатася соинытснтя ссоа снала аоаь с у ооньлаоносоаа ааоота нрьнеа ь соылоеори от рыасиаоаь оно оа сласто ь о оаонткн оть ссосоноостьь лоиьсн ето соаонсооа стол аоаа с сньта тн ота ьоа ьтсиа нтио сосал аооо оаонььсоаьть оно калт соояьа ст он етоь роониа сслосаи ь оо сосалорись он стриа ыал н с лроа ытьькрусь слоналор а аео еирнооо саыькаа еинон лныостоо онниастниа слаоон тася ротсоо кто ст сосал аооо снатт с тоиор сонони оо слаоонуся ьоа сиаыоснио нт онстнсатя снс онсаснтя он етоь он иастокоа нрьнеа а соысаснтясь сокаьр сотоьр кто калао сьтя ьаорт ст сон ата кто сса ето оаонткн оо слосто'"
      ]
     },
     "execution_count": 1705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_mapping_bigram = get_reverse_mapping_bigram(corpus_freqs_bigram, text_freqs_bigram, init_mapping=init_mapping)\n",
    "decoded_text = apply_mapping(encoded_text, reverse_mapping_bigram)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36045197740112994"
      ]
     },
     "execution_count": 1706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(tokenized_text, decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики выросли, но прочесть это все так же невозможно :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Метод на основе MCMC-семплирования\n",
    "\n",
    "Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "- предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "- реализуйте и протестируйте его, убедитесь, что результаты улучшились.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст, разбитый на биграммы - это, по сути, марковская цепь. Частота биграмм - вероятность перехода по цепи.\n",
    "\n",
    "Для обучения перестановки будем считать вероятность порождения именно такого текста как произведение вероятностей всех биграмм, в него входящих. \n",
    "\n",
    "Всего перестановок очень много, поэтому сделаем жадный алгоритм, который использует идею MCMC-семплирования.\n",
    "\n",
    "Алгоритм:\n",
    "\n",
    "0) Инициализируем перестановки, восстанавливаем текст и считаем на нем $p_{current}$ \n",
    "\n",
    "1) Меняем местами пару букв для перестановки\n",
    "\n",
    "2) Восстанавливаем текст с новой перестановкой и считаем на нем $p_{proposal}$  \n",
    "\n",
    "3) Принимаем новую перестановку с \"вероятностью\" $p_{accept} = \\frac{p_{proposal}}{p_{current}}$ \n",
    "\n",
    "4) Возвращаемся к пункту 1\n",
    "\n",
    "Мы можем уйти немного не туда и застрять не в том максимуме, поэтому будем делать много попыток и брать лучшую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1758,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freqs_smooth(text, min_freq=0, n_gram=2):\n",
    "    freqs = dict()\n",
    "    vocab_len = len(set(text))**n_gram\n",
    "    if n_gram > 1:\n",
    "        text = [''.join(ngram) for ngram in everygrams(text, min_len=n_gram, max_len=n_gram)]\n",
    "    for key, value in Counter(text).items():\n",
    "        freqs[key] = (value + 1) / (len(text) + vocab_len) # Сглаживаем, чтобы не было нулей\n",
    "    return freqs\n",
    "\n",
    "def get_text_proba(text, mapping, freqs, n_gram=2):\n",
    "    decoded_text = apply_mapping(text, mapping)\n",
    "    log_proba = 0\n",
    "    for i in range(len(decoded_text) - n_gram):\n",
    "        bigram = decoded_text[i: i + n_gram]\n",
    "        bigram_proba = freqs.get(bigram)\n",
    "        if bigram_proba is None:\n",
    "            bigram_proba = 1 / (len(text) + len(abc)**n_gram) # Сглаживаем, чтобы не было нулей\n",
    "            \n",
    "        log_proba += np.log(bigram_proba)\n",
    "    return log_proba\n",
    "        \n",
    "def get_reverse_mapping_mcmc(encoded_text, abc_encoded, abc_corpus, freqs_corpus, n_iters=10000, n_trials=10, n_gram=2):\n",
    "\n",
    "    accept_count = 0\n",
    "    best_mapping = None\n",
    "    all_mappings = []\n",
    "    best_log_likekihood = -np.inf\n",
    "\n",
    "    for trial in tqdm(range(n_trials), leave=False, position=0, total=n_trials):\n",
    "\n",
    "        abc_encoded = list(abc_encoded)\n",
    "        abc_iter = list(abc_corpus)\n",
    "        reverse_mapping = {k: v for k, v in zip(abc_encoded, abc_iter[:len(abc_encoded)])}\n",
    "        log_proba_current = get_text_proba(encoded_text, reverse_mapping, freqs_corpus, n_gram=n_gram)\n",
    "\n",
    "        for i in range(n_iters):\n",
    "            abc_proposal = abc_iter[:]\n",
    "            idx1, idx2 = np.random.choice(len(abc_proposal), replace=False, size=2)\n",
    "            abc_proposal[idx1], abc_proposal[idx2] = abc_proposal[idx2], abc_proposal[idx1]\n",
    "            reverse_mapping_proposal = {k: v for k, v in zip(abc_encoded, abc_proposal[:len(abc_encoded)])}\n",
    "            log_proba_proposal = get_text_proba(encoded_text, reverse_mapping_proposal, freqs_corpus, n_gram=n_gram)\n",
    "\n",
    "            p_accept = np.exp(log_proba_proposal - log_proba_current)\n",
    "\n",
    "            if p_accept > np.random.rand():\n",
    "                accept_count += 1\n",
    "                abc_iter = abc_proposal\n",
    "                log_proba_current = log_proba_proposal\n",
    "                reverse_mapping = reverse_mapping_proposal\n",
    "\n",
    "        if log_proba_current > best_log_likekihood:\n",
    "            best_log_likekihood = log_proba_current\n",
    "            best_mapping = reverse_mapping\n",
    "            \n",
    "        all_mappings.append(reverse_mapping)\n",
    "\n",
    "\n",
    "    print(f'Best likelihood: {best_log_likekihood}')        \n",
    "    print(f'Accept raito: {accept_count / (n_iters * n_trials)}')\n",
    "    return best_mapping, all_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1740,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_corpus = get_freqs_smooth(tokenized_corpus, n_gram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 best likelihood: -5649.4222552997235\n",
      "Trial 1 best likelihood: -4983.6918053880245\n",
      "Trial 2 best likelihood: -5726.083384780235\n",
      "Trial 3 best likelihood: -4986.237995189518\n",
      "Trial 4 best likelihood: -5661.413882794688\n",
      "Trial 5 best likelihood: -5033.768741636718\n",
      "Trial 6 best likelihood: -4982.122522947728\n",
      "Trial 7 best likelihood: -4983.6918053880245\n",
      "Trial 8 best likelihood: -5591.465892426024\n",
      "Trial 9 best likelihood: -5425.1575527392115\n",
      "Best likelihood: -4982.122522947728\n",
      "Accept raito: 0.01232\n"
     ]
    }
   ],
   "source": [
    "best_reverse_mapping, _ = get_reverse_mapping_mcmc(\n",
    "    encoded_text, \n",
    "    abc_encoded=abc, \n",
    "    abc_corpus=abc,\n",
    "    freqs_corpus=freqs_corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в течение многих часов шерлок холмс сидел согнувшись над стеклянной пробиркой в которой варилось чтото на редкость вонючее голова его была опуъена на грудь и он казался мне похожим на странную товцую птицу с тусклыми серыми перьями и черным хохолком итак уотсон сказал он внезапно вы не собираетесь вкладывать свои сбережения в южноафриканские ценные бумаги я вздрогнул от удивления как ни привык я к необычайным способностям холмса это внезапное вторжение в самые тайные мои мысли было совершенно необщяснимым как черт возьми вы об этом узнали спросил я он повернулся на стуле держа в руке дымяъуюся пробирку и его глубоко сидяъие глаза радостно заблистали признайтесь уотсон что вы совершенно сбиты с толку сказал он признаюсь мне следовало бы заставить вас написать об этом на листочке бумаги и подписаться почему потому что через пять минут вы скажете что все это необычайно просто'"
      ]
     },
     "execution_count": 1577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_mapping(encoded_text, best_reverse_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Расшифруйте сообщение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"←⇠⇒↟↹↷⇊↹↷↟↤↟↨←↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↟⇒↟↹⇷⇛⇞↨↟↹↝⇛⇯↳⇴⇒⇈↝⇊↾↹↨←⇌⇠↨↹⇙↹⇸↨⇛↙⇛↹⇠⇛⇛↲⇆←↝↟↞↹⇌⇛↨⇛⇯⇊↾↹⇒←↙⇌⇛↹⇷⇯⇛⇞↟↨⇴↨⇈↹⇠⇌⇛⇯←←↹↷⇠←↙⇛↹↷⇊↹↷⇠←↹⇠↤←⇒⇴⇒↟↹⇷⇯⇴↷↟⇒⇈↝⇛↹↟↹⇷⇛⇒⇙⇞↟↨←↹↳⇴⇌⇠↟↳⇴⇒⇈↝⇊↾↹↲⇴⇒⇒↹⇰⇴↹⇷⇛⇠⇒←↤↝←←↹⇞←↨↷←⇯↨⇛←↹⇰⇴↤⇴↝↟←↹⇌⇙⇯⇠⇴↹↘⇛↨↞↹⇌⇛↝←⇞↝⇛↹↞↹↝↟⇞←↙⇛↹↝←↹⇛↲←⇆⇴⇏\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' оеанитслвркдмупяьгыбзчжйшхюэцщфъ', '↹←⇛↟⇒↝⇴↨⇠⇯↷⇌⇊⇞⇈⇷↤↳↾↙⇙↲↞⇆⇰⇸↘⇏')"
      ]
     },
     "execution_count": 1741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_freqs_sorted = sorted(corpus_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "message_freqs_sorted = sorted(message_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "abc_corpus = ''.join([c for c, _ in corpus_freqs_sorted])\n",
    "abc_message = ''.join([c for c, _ in message_freqs_sorted])\n",
    "abc_corpus, abc_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1742,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_corpus = get_freqs_smooth(tokenized_corpus, n_gram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1747,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best likelihood: -1231.2547551943435\n",
      "Accept raito: 0.042858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_reverse_mapping, _ = get_reverse_mapping_mcmc(\n",
    "    message, \n",
    "    abc_encoded=abc_message, \n",
    "    abc_corpus=abc_corpus,\n",
    "    freqs_corpus=freqs_corpus,\n",
    "    n_iters=10000,\n",
    "    n_trials=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1748,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'если вы вимите нордальный или почти нордальный текст у этого соожшения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный жалл за послемнее четвертое замание курса ботя конечно я ничего не ожешаю'"
      ]
     },
     "execution_count": 1748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_mapping(message, best_reverse_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Успех :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9391304347826087"
      ]
     },
     "execution_count": 1753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_message = \"если вы видите нормальный или почти нормальный текст у этого сообщения который легко прочитать скорее всего вы все сделали правильно и получите максимальный балл за последнее четвертое задание курса хотя конечно я ничего не обещаю\"\n",
    "encoded_message = apply_mapping(message, best_reverse_mapping)\n",
    "char_accuracy(original_message, encoded_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A что если от биграмм перейти к триграммам\n",
    "\n",
    "Бонус: а что если от биграмм перейти к триграммам (тройкам букв) или даже больше? Улучшатся ли результаты? Когда улучшатся, а когда нет? Чтобы ответить на этот вопрос эмпирически, уже может понадобиться погенерировать много тестовых перестановок и последить за метриками, глазами может быть и не видно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Попробуем раскодировать сообщение из пункта 4 триграммами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' оеанитслвркдмупяьгыбзчжйшхюэцщфъ', '↹←⇛↟⇒↝⇴↨⇠⇯↷⇌⇊⇞⇈⇷↤↳↾↙⇙↲↞⇆⇰⇸↘⇏')"
      ]
     },
     "execution_count": 1763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_freqs_sorted = sorted(corpus_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "message_freqs_sorted = sorted(message_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "abc_corpus = ''.join([c for c, _ in corpus_freqs_sorted])\n",
    "abc_message = ''.join([c for c, _ in message_freqs_sorted])\n",
    "abc_corpus, abc_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1764,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_corpus_trigramm = get_freqs_smooth(tokenized_corpus, n_gram=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1769,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best likelihood: -1725.6602016838426\n",
      "Accept raito: 0.044895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_reverse_mapping, _ = get_reverse_mapping_mcmc(\n",
    "    message, \n",
    "    abc_encoded=abc_message, \n",
    "    abc_corpus=abc_corpus,\n",
    "    freqs_corpus=freqs_corpus_trigramm,\n",
    "    n_gram=3,\n",
    "    n_iters=10000,\n",
    "    n_trials=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1770,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'если вы видите нормальный или почти нормальный текст у этого сообщения который легко прочитать скорее всего вы все сделали правильно и получите максимальный балл за последнее четвертое задание курса хотя конечно я ничего не обещаш'"
      ]
     },
     "execution_count": 1770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_mapping(message, best_reverse_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1771,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956521739130435"
      ]
     },
     "execution_count": 1771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_message = \"если вы видите нормальный или почти нормальный текст у этого сообщения который легко прочитать скорее всего вы все сделали правильно и получите максимальный балл за последнее четвертое задание курса хотя конечно я ничего не обещаю\"\n",
    "encoded_message = apply_mapping(message, best_reverse_mapping)\n",
    "char_accuracy(original_message, encoded_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось почти идеально, качество выросло. \n",
    "\n",
    "**2. Попробуем тексты подлиннее, может что-то пойдет не так**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' оеанитслвркдмупяьгыбзчжйшхюэцщфъ', 'гуъпеюиофбтцр ыядшьскжзмлэвйчанхщ')"
      ]
     },
     "execution_count": 1778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = generate_mapping(corpus_freqs)\n",
    "encoded_text = apply_mapping(tokenized_text, mapping)\n",
    "text_freqs = get_freqs(encoded_text)\n",
    "\n",
    "corpus_freqs_sorted = sorted(corpus_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "text_freqs_sorted = sorted(text_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "abc_corpus = ''.join([c for c, _ in corpus_freqs_sorted])\n",
    "abc_text = ''.join([c for c, _ in text_freqs_sorted])\n",
    "abc_corpus, abc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1785,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best likelihood: -4991.84417224061\n",
      "Accept raito: 0.00686\n",
      "-----\n",
      "фгопьпъипгцъузилгьюеуфгйпбтурглутцегеижптгеузъ фйиесгъюжгеопртшъъумгябудибрумгфгруоубумгфюбитуесгьоуоугъюгбпжруеосгфуъэьппгзутуфюгпзугдытюгуя апъюгъюгзб жсгигуъгрюкютешгцъпгяулувицгъюгеобюъъ эгоуфн эгяоин гего ертыцигепбыцигяпбсшцигигьпбъыцглулутруцгиоюрг уоеуъгерюкютгуъгфъпкюяъугфыгъпгеудибюпопесгфртюжыфюосгефуигедпбпвпъишгфгэвъуюхбирюъерипгнпъъыпгд цюзигшгфкжбузъ тгуог жифтпъишгрюргъигябифыргшгргъпудыьюмъыцгеяуеудъуеошцглутцеюгчоугфъпкюяъупгфоубвпъипгфгеюцыпгоюмъыпгцуигцыетигдытугеуфпбйпъъугъпудщшеъицыцгрюргьпбогфуксцигфыгудгчоуцг къютигеябуеитгшгуъгяуфпбъ тешгъюгео тпгжпбвюгфгб рпгжыцша эешгябудибр гигпзугзт дуругеижшаипгзтюкюгбюжуеоъугкюдтиеоютигябикъюмопесг уоеуъгьоугфыгеуфпбйпъъугедиоыгегоутр герюкютгуъгябикъюэесгцъпгетпжуфютугдыгкюеоюфиосгфюегъюяиеюосгудгчоуцгъюгтиеоуьрпгд цюзигигяужяиеюосешгяуьпц гяуоуц гьоугьпбпкгяшосгциъ огфыгерювпопгьоугфепгчоугъпудыьюмъугябуеоу\n",
      "-----\n",
      "в течение многих часов шерлок холмс сидел согнувшись над стеклянной пробиркой в которой варилось чтото на редкость вонючее голова его была опуъена на грудь и он казался мне похожим на странную товцую птицу с тусклыми серыми перьями и черным хохолком итак уотсон сказал он внезапно вы не собираетесь вкладывать свои сбережения в южноафриканские ценные бумаги я вздрогнул от удивления как ни привык я к необычайным способностям холмса это внезапное вторжение в самые тайные мои мысли было совершенно необщяснимым как черт возьми вы об этом узнали спросил я он повернулся на стуле держа в руке дымяъуюся пробирку и его глубоко сидяъие глаза радостно заблистали признайтесь уотсон что вы совершенно сбиты с толку сказал он признаюсь мне следовало бы заставить вас написать об этом на листочке бумаги и подписаться почему потому что через пять минут вы скажете что все это необычайно просто\n",
      "-----\n",
      "0.9954802259887006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_reverse_mapping, _ = get_reverse_mapping_mcmc(\n",
    "    encoded_text, \n",
    "    abc_encoded=abc_text, \n",
    "    abc_corpus=abc_corpus,\n",
    "    freqs_corpus=freqs_corpus,\n",
    "    n_gram=2,\n",
    "    n_iters=10000,\n",
    "    n_trials=5,\n",
    ")\n",
    "print('-----')\n",
    "print(encoded_text)\n",
    "print('-----')\n",
    "decoded_text = apply_mapping(encoded_text, best_reverse_mapping)\n",
    "print(decoded_text)\n",
    "print('-----')\n",
    "print(char_accuracy(tokenized_text, decoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1786,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best likelihood: -6859.4582829474975\n",
      "Accept raito: 0.00766\n",
      "-----\n",
      "фгопьпъипгцъузилгьюеуфгйпбтурглутцегеижптгеузъ фйиесгъюжгеопртшъъумгябудибрумгфгруоубумгфюбитуесгьоуоугъюгбпжруеосгфуъэьппгзутуфюгпзугдытюгуя апъюгъюгзб жсгигуъгрюкютешгцъпгяулувицгъюгеобюъъ эгоуфн эгяоин гего ертыцигепбыцигяпбсшцигигьпбъыцглулутруцгиоюрг уоеуъгерюкютгуъгфъпкюяъугфыгъпгеудибюпопесгфртюжыфюосгефуигедпбпвпъишгфгэвъуюхбирюъерипгнпъъыпгд цюзигшгфкжбузъ тгуог жифтпъишгрюргъигябифыргшгргъпудыьюмъыцгеяуеудъуеошцглутцеюгчоугфъпкюяъупгфоубвпъипгфгеюцыпгоюмъыпгцуигцыетигдытугеуфпбйпъъугъпудщшеъицыцгрюргьпбогфуксцигфыгудгчоуцг къютигеябуеитгшгуъгяуфпбъ тешгъюгео тпгжпбвюгфгб рпгжыцша эешгябудибр гигпзугзт дуругеижшаипгзтюкюгбюжуеоъугкюдтиеоютигябикъюмопесг уоеуъгьоугфыгеуфпбйпъъугедиоыгегоутр герюкютгуъгябикъюэесгцъпгетпжуфютугдыгкюеоюфиосгфюегъюяиеюосгудгчоуцгъюгтиеоуьрпгд цюзигигяужяиеюосешгяуьпц гяуоуц гьоугьпбпкгяшосгциъ огфыгерювпопгьоугфепгчоугъпудыьюмъугябуеоу\n",
      "-----\n",
      "в течение многих часов шерлок холмс сидел согнувшись над стеклянной пробиркой в которой варилось чтото на редкость вонючее голова его была опущена на грудь и он казался мне похожим на странную товъую птиъу с тусклыми серыми перьями и черным хохолком итак уотсон сказал он внезапно вы не собираетесь вкладывать свои сбережения в южноацриканские ъенные бумаги я вздрогнул от удивления как ни привык я к необычайным способностям холмса это внезапное вторжение в самые тайные мои мысли было совершенно необфяснимым как черт возьми вы об этом узнали спросил я он повернулся на стуле держа в руке дымящуюся пробирку и его глубоко сидящие глаза радостно заблистали признайтесь уотсон что вы совершенно сбиты с толку сказал он признаюсь мне следовало бы заставить вас написать об этом на листочке бумаги и подписаться почему потому что через пять минут вы скажете что все это необычайно просто\n",
      "-----\n",
      "0.9943502824858758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_reverse_mapping, _ = get_reverse_mapping_mcmc(\n",
    "    encoded_text, \n",
    "    abc_encoded=abc_text, \n",
    "    abc_corpus=abc_corpus,\n",
    "    freqs_corpus=freqs_corpus_trigramm,\n",
    "    n_gram=3,\n",
    "    n_iters=10000,\n",
    "    n_trials=5,\n",
    ")\n",
    "\n",
    "print('-----')\n",
    "print(encoded_text)\n",
    "print('-----')\n",
    "decoded_text = apply_mapping(encoded_text, best_reverse_mapping)\n",
    "print(decoded_text)\n",
    "print('-----')\n",
    "print(char_accuracy(tokenized_text, decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество практически не изменилось.\n",
    "\n",
    "**3. Попробуем еще одну перестановку, но уже с очень коротким текстом**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' оеанитслвркдмупяьгыбзчжйшхюэцщфъ', 'убефсымвогхпит шцьчзрнжйящюа')"
      ]
     },
     "execution_count": 1794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_text = 'это очень короткий текст чтобы проверить гипотезу что совсем маленькие тексты раскодируются последовательностями из двух букв хуже чем из трех'\n",
    "\n",
    "mapping = generate_mapping(corpus_freqs)\n",
    "encoded_text = apply_mapping(short_text, mapping)\n",
    "text_freqs = get_freqs(encoded_text)\n",
    "\n",
    "corpus_freqs_sorted = sorted(corpus_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "text_freqs_sorted = sorted(text_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "abc_corpus = ''.join([c for c, _ in corpus_freqs_sorted])\n",
    "abc_text = ''.join([c for c, _ in text_freqs_sorted])\n",
    "abc_corpus, abc_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Биграммы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1795,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best likelihood: -795.9068114645025\n",
      "Accept raito: 0.04672\n",
      "-----\n",
      "йбеуехфтпумевебмсяубфмыбухберну веофвсбпущс ебфшгухбеуыеоыфиуицьфтпмсфубфмыбнувцымечсвгюбыжу еыьфчеоцбфьптеыбжисусшучогзургмоузгафухфиусшубвфз\n",
      "-----\n",
      "щсо одела тоностих сетвс дсочь кнорениса эикосейы дсо ворвем мушелатие сетвсь нувтопиныъсвя ковшепорусешаловсями ий прыб чытр быюе дем ий снеб\n",
      "-----\n",
      "0.4154929577464789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_reverse_mapping, _ = get_reverse_mapping_mcmc(\n",
    "    encoded_text, \n",
    "    abc_encoded=abc_text, \n",
    "    abc_corpus=abc_corpus,\n",
    "    freqs_corpus=freqs_corpus,\n",
    "    n_gram=2,\n",
    "    n_iters=10000,\n",
    "    n_trials=5,\n",
    ")\n",
    "print('-----')\n",
    "print(encoded_text)\n",
    "print('-----')\n",
    "decoded_text = apply_mapping(encoded_text, best_reverse_mapping)\n",
    "print(decoded_text)\n",
    "print('-----')\n",
    "print(char_accuracy(short_text, decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Триграммы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1796,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best likelihood: -1126.9021005498848\n",
      "Accept raito: 0.0466\n",
      "-----\n",
      "йбеуехфтпумевебмсяубфмыбухберну веофвсбпущс ебфшгухбеуыеоыфиуицьфтпмсфубфмыбнувцымечсвгюбыжу еыьфчеоцбфьптеыбжисусшучогзургмоузгафухфиусшубвфз\n",
      "-----\n",
      "это очерь новотний тенст чтому пводевить бипотеха что содсел лышерьние тенсту высногиваются посшегодытешьростяли их гдак манд каже чел их твек\n",
      "-----\n",
      "0.647887323943662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_reverse_mapping, _ = get_reverse_mapping_mcmc(\n",
    "    encoded_text, \n",
    "    abc_encoded=abc_text, \n",
    "    abc_corpus=abc_corpus,\n",
    "    freqs_corpus=freqs_corpus_trigramm,\n",
    "    n_gram=3,\n",
    "    n_iters=10000,\n",
    "    n_trials=5,\n",
    ")\n",
    "\n",
    "print('-----')\n",
    "print(encoded_text)\n",
    "print('-----')\n",
    "decoded_text = apply_mapping(encoded_text, best_reverse_mapping)\n",
    "print(decoded_text)\n",
    "print('-----')\n",
    "print(char_accuracy(short_text, decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-граммы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1797,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_corpus_fourgramm = get_freqs_smooth(tokenized_corpus, n_gram=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1798,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best likelihood: -1412.4571300090768\n",
      "Accept raito: 0.05224\n",
      "-----\n",
      "йбеуехфтпумевебмсяубфмыбухберну веофвсбпущс ебфшгухбеуыеоыфиуицьфтпмсфубфмыбнувцымечсвгюбыжу еыьфчеоцбфьптеыбжисусшучогзургмоузгафухфиусшубвфз\n",
      "-----\n",
      "это очень короткий текст чтобы проверить гипотезу что совсем маленькие тексты раскодируются последовательностями из двух букв хуже чем из трех\n",
      "-----\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "best_reverse_mapping, _ = get_reverse_mapping_mcmc(\n",
    "    encoded_text, \n",
    "    abc_encoded=abc_text, \n",
    "    abc_corpus=abc_corpus,\n",
    "    freqs_corpus=freqs_corpus_fourgramm,\n",
    "    n_gram=4,\n",
    "    n_iters=10000,\n",
    "    n_trials=5,\n",
    ")\n",
    "\n",
    "print('-----')\n",
    "print(encoded_text)\n",
    "print('-----')\n",
    "decoded_text = apply_mapping(encoded_text, best_reverse_mapping)\n",
    "print(decoded_text)\n",
    "print('-----')\n",
    "print(char_accuracy(short_text, decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось просто идеально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдение по n-граммам**\n",
    "\n",
    "Похоже на то, что триграммы дают хороший прирост качества для декодирования коротких текстов. Для длинных текстов в принципе и так все хорошо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Применения для этой модели\n",
    "\n",
    "Бонус: какие вы можете придумать применения для этой модели? Пляшущие человечки ведь не так часто встречаются в жизни (хотя встречаются! и это самое потрясающее во всей этой истории, но об этом я расскажу потом)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я пока что не в курсе конкретных задач биоинформатики, но подозреваю, что такие модели могут использоваться для работы с последовательностями ДНК. Там число вариаций просто огромно и явно больше чем число жителей земного шара, и, думаю, обычные методы здесь не подходят. Скорее всего, одно из применений - как раз в анализе последовательностей ДНК в биоинформатике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
